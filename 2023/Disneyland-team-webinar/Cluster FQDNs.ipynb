{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a668980c-705d-4202-bfbd-9ce65f3706f9",
   "metadata": {},
   "source": [
    "# Clustering FQDNs\n",
    "\n",
    "Now that we have the data from DNSDB, we'd like to see if we can repeat the clustering, but on the fqdns. \n",
    "\n",
    "There's two ways to do this clustering: \n",
    " 1. Cluster fqdns like they're documents, clustering each one as a separate document\n",
    " 1. Group subdomains together by their base registrable domain, cluster the registrable domains together by the pattern in their subdomains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b28e9ba6-b2b1-41fc-83cd-e8e0bd1b7589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageFilter\n",
    "import idna\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af1d734-dcb0-41e1-8c89-1a464a9a4b70",
   "metadata": {},
   "source": [
    "## Attempt 1 - cluster fqdns together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23625db7-471d-4d57-bc2a-f8b0ef639ea2",
   "metadata": {},
   "source": [
    "## Step 1 - load the clusters & make the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7ce2939-d232-407e-9aa3-e83dd4c045bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_clusters = dict()\n",
    "cluster_ids = list()\n",
    "with open(\"disney-clusters.json\") as infile:\n",
    "    data = json.load(infile)\n",
    "    for cluster_id in data:\n",
    "        if cluster_id == -1:\n",
    "            # skip the grab bad\n",
    "            continue\n",
    "        cluster_text = f\"cluster_id-{cluster_id}\"\n",
    "        cluster_ids.append(cluster_text)\n",
    "        domains = data[cluster_id]\n",
    "        for domain in domains:\n",
    "            domain_clusters[domain] = cluster_text\n",
    "        #if cluster_id != -1:\n",
    "        #    domains = data[cluster_id]\n",
    "        #    domain_id = domains[0]\n",
    "        #    for domain in domains:\n",
    "        #        domain_clusters[domain] = domain_id\n",
    "        #else:\n",
    "        #    # these are ones that failed to cluster, leave them as their originals\n",
    "        #    for broken in data[cluster_id]:\n",
    "        #        domain_clusters[broken] = broken\n",
    "stop_words = list(ENGLISH_STOP_WORDS.union(cluster_ids))\n",
    "cluster_stop_words = list(cluster_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35cc0c83-dc79-4e00-a6e9-3dab93e269bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['26uy6.top',\n",
       " 'appchasses6wqwb9nzykac95ng.com',\n",
       " 'apple-unlocked.com',\n",
       " 'appsuptmpjubxee3gcvimqe6mq3rp.com',\n",
       " 'bridgeaccess.us',\n",
       " 'cashappmvb-sec.com',\n",
       " 'cashappsalert-sec.com',\n",
       " 'commercialservlces-mtb.com',\n",
       " 'cprapid.com',\n",
       " 'designestylelab.com']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(domain_clusters.keys())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681287e2-6afc-4e91-ab2d-2e27dad62b29",
   "metadata": {},
   "source": [
    "## Step 2 - load the FQDNs from dnsdb & replace their base registrable domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ea84acd-2616-4dfe-9382-fcf47aa2686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fqdns = list()\n",
    "def decode_domain(entry):\n",
    "    try:\n",
    "        decoded = idna.decode(entry)\n",
    "        if decoded.endswith(\".\"):\n",
    "            decoded = decoded.rstrip(\".\")\n",
    "        return decoded\n",
    "    except (idna.InvalidCodepoint, idna.IDNAError):\n",
    "        return None\n",
    "\n",
    "with open(\"dnsdb_resolutions.json\") as infile:\n",
    "    data = json.load(infile)\n",
    "    for base_domain in data:\n",
    "        apex_domains = set()\n",
    "        # apex_domains = {decode_domain(entry['domain']) for entry in data[base_domain]['apex']}\n",
    "        subdomains = {decode_domain(entry['domain']) for entry in data[base_domain]['subdomains']}\n",
    "        all_domains = apex_domains.union(subdomains)\n",
    "        if len(all_domains) > 20:\n",
    "            continue\n",
    "        if None in all_domains:\n",
    "            all_domains.remove(None)\n",
    "        fqdns.extend(list(all_domains))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ea4c5ef-dd89-4c28-983b-28c65f948fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_fqdns = list()\n",
    "keys = list(domain_clusters.keys())\n",
    "for entry in fqdns:\n",
    "    found = False\n",
    "    counter = 0\n",
    "    while not found and counter < len(domain_clusters):\n",
    "        key = keys[counter]\n",
    "        if entry.endswith(\".\" + key):\n",
    "            # key is the registrable domain\n",
    "            fixed_fqdn = entry.replace(key, domain_clusters[key])\n",
    "            fixed_fqdns.append(fixed_fqdn)\n",
    "            found = True\n",
    "        elif entry == key:\n",
    "            # skip these. only interested in ones with subdomains\n",
    "            # fixed_fqdns.append(domain_clusters[entry])\n",
    "            found = True\n",
    "        counter += 1\n",
    "    if not found:\n",
    "        # means we made it through without doing a substitution, which is surprising\n",
    "        print(f\"didn't find {entry}\")\n",
    "        fixed_fqdns.append(entry)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fcd4d63-56e4-4eeb-84f4-a2ded05a2b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['singlepoint.cluster_id-12',\n",
       " 'singlepoint.cluster_id-12',\n",
       " 'singlepoint.cluster_id-12',\n",
       " 'www.cluster_id-12',\n",
       " 'www.cluster_id-12',\n",
       " 'singlepoint.cluster_id-12',\n",
       " 'mail.cluster_id--1',\n",
       " 'www.cluster_id--1',\n",
       " 'www.cluster_id-56',\n",
       " 'www.cluster_id-56']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_fqdns[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77470c12-bf76-482a-bd09-676bb8ad0851",
   "metadata": {},
   "source": [
    "### step 2.5: remove duplicate domains after normalization like this (otherwise they'll all just cluster with themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0b15180-4b1e-4f4b-b4b1-2ce1b2c3ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_fqdns = set(fixed_fqdns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4bed90-2d14-45ff-bed0-6ca677f38a4f",
   "metadata": {},
   "source": [
    "## Step 3 - break them apart into mini documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ce59bde-1b43-487f-94fa-f78aa243a013",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\" \".join(re.split(\"[.]\", entry)) for entry in fixed_fqdns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85d78ab8-52b5-470b-bb96-81372112480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other idea for the documents: make one big document per cluster-id, see if we find commonalities that way.\n",
    "big_docs = defaultdict(list)\n",
    "for fqdn in fixed_fqdns:\n",
    "    minidoc = re.split(\"[.]\", fqdn)\n",
    "    cluster = fqdn[-1]\n",
    "    big_docs[cluster].extend(minidoc)\n",
    "big_documents = [\" \".join(entry) for entry in big_docs.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43707f38-d6ad-4731-8311-4ebff625c1eb",
   "metadata": {},
   "source": [
    "## Step 4 - do normal clustering on them like they were documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "279060e0-fda8-4c88-8918-265eb2bac6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = cluster_stop_words)\n",
    "X_tfidf = vectorizer.fit_transform(big_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0440352d-7622-4e3a-8758-af4b0b5b763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_vectorizer = HashingVectorizer()\n",
    "X_hash = hash_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "643e3b08-4bd5-4ca5-a91c-461ad7a0c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "X_count = count_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ecdb8d-4185-4089-9c6d-fa476f2c778d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbb0c171-2ef1-41e3-bd64-18c5357073c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=0.7, min_samples=2).fit(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a85c11f-c816-4684-afb0-cfe7213fd7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 0\n",
      "Estimated number of noise points: 10\n"
     ]
    }
   ],
   "source": [
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31b1a6c6-ae47-4d8e-b21b-c51b79df806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated = defaultdict(list)\n",
    "for label, entry in zip(labels, documents):\n",
    "    translated[int(label)].append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82005eff-3927-4030-9d86-89d973d9e69f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster -1\n",
      "\tbanking cluster_id-47\n",
      "\tbfwkoiy cluster_id-28\n",
      "\tcpanel cluster_id--1\n",
      "\tcpcontacts cluster_id-8\n",
      "\thelp cluster_id-0\n",
      "\tro cluster_id-46\n",
      "\twebdisk cluster_id-14\n",
      "\twebmail cluster_id-14\n",
      "\twww cluster_id-8\n",
      "\twww treasury cluster_id-53\n"
     ]
    }
   ],
   "source": [
    "for key in translated:\n",
    "    print(f\"cluster {key}\")\n",
    "    for entry in sorted(translated[key]):\n",
    "        print(f\"\\t{entry}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0051d9-de7e-4e0d-9501-e65d1c514258",
   "metadata": {},
   "source": [
    "Okay, that's not doing what I want. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad3bcb2-8f59-4392-a404-4b0fa6b56f51",
   "metadata": {},
   "source": [
    "# Attempt 2 - cluster registrable domains by their subdomain words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b639c43-bdc0-45fa-b310-1707f08827a0",
   "metadata": {},
   "source": [
    "Let's try this a different way. Let's take the domains from the dnsdb_resolutions, and simply pull all the subdomains found for a given registrable domain, and make them all one document. So, [`www.test.com`, `mail.test.com`, `secure.test.com`, `login.test.com`] would collapse to a document like `www mail secure login test.com`. That should allow us to cluster the domains together by their subdomains to see if there are common patterns in the subdomain registration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e45f5b-ff6d-4e71-b8cc-c0ae1d66035c",
   "metadata": {},
   "source": [
    "Notes about the document creation here: \n",
    " 1. I'm going to leave in the original registrable domain for the sake of interpretability\n",
    " 1. I'm going to defang that registrable domain by replacing the \".\" with \"_\". If I don't, some of the tfidf parsing code will split the domain name up, and treat \"com\" as a word also. Don't want that. \n",
    " 1. This shouldn't matter since we're treating each registerable domain separately, but we don't want to cluster on the registrable domains themselves, just the word patterns underneath them, so I'm also adding the registrable domain itself as a stop word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d5b1639-ca99-4f27-9d09-87d275c6ab82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fqdn_docs = list()\n",
    "# start with \"www\" as a stop word. Don't feel like I learn anything from a domain having that as a subdomain\n",
    "stop_domains = [\"www\"]\n",
    "with open(\"dnsdb_resolutions.json\") as infile:\n",
    "    data = json.load(infile)\n",
    "    for base_domain in data:\n",
    "        if base_domain.endswith(\".\"):\n",
    "            substitute_domain = base_domain.rstrip(\".\")\n",
    "            stop_domain = base_domain.rstrip(\".\").replace(\".\", \"_\")\n",
    "        else:\n",
    "            stop_domain = base_domain.replace(\".\", \"_\")\n",
    "            substitute_domain = base_domain\n",
    "        stop_domains.append(stop_domain)\n",
    "        apex_domains = set()\n",
    "        # apex_domains = {decode_domain(entry['domain']) for entry in data[base_domain]['apex']}\n",
    "        subdomains = {decode_domain(entry['domain']) for entry in data[base_domain]['subdomains']}\n",
    "        all_domains = apex_domains.union(subdomains)\n",
    "        if len(all_domains) > 20:\n",
    "            continue\n",
    "        if None in all_domains:\n",
    "            all_domains.remove(None)\n",
    "        words = set()\n",
    "        for entry in all_domains:\n",
    "            subdomains = entry.replace(substitute_domain, \"\")\n",
    "            if not subdomains: \n",
    "                continue\n",
    "            words.update([word for word in subdomains.split(\".\") if word])\n",
    "        if not words:\n",
    "            continue\n",
    "        words.add(stop_domain)\n",
    "        fqdn_docs.append(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ff81eb2-755b-4b22-882f-74af874b6e85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images whm vb blog i2 ww sandbox www vb5 53vb_com',\n",
       " 'www 53vl_com m ww1',\n",
       " 'www 53vz_com']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fqdn_docs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b4e22-f90a-4ef8-a04f-9a7bc79989f3",
   "metadata": {},
   "source": [
    "I'm concerned that the documents that are nothing but stop words (just the registrable domain, or \"www\" + the domain) are causing problems for the clustering app. Let's try removing those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fc86aaf-f84b-457e-88bf-b99a92714731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"www\" in stop_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4673adcf-ce90-4f8d-9f64-38ce32a5763e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'www 53vz_com'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fqdn_docs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6085bae-ce08-48f6-9d38-90184e20730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in fqdn_docs[2].split(\" \"):\n",
    "    if word not in stop_domains:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f80da1e-ea45-4efa-b0d8-8e003d38828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_fqdn_docs = list()\n",
    "for doc in fqdn_docs:\n",
    "    # test if all of the words for the doc are in stop_words    \n",
    "    for word in doc.split(\" \"):\n",
    "        if word not in stop_domains:\n",
    "            cleaned_fqdn_docs.append(doc)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1558ae0-56ac-47af-b7a1-458e4f29810f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images whm vb blog i2 ww sandbox www vb5 53vb_com',\n",
       " 'www 53vl_com m ww1',\n",
       " 'cpanel wordpress remote app www 53xa_com test']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_fqdn_docs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "213a4c03-13e1-4874-af93-60aace29324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_domains)\n",
    "X_tfidf = vectorizer.fit_transform(cleaned_fqdn_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77bc76e0-710e-4036-bc23-6099cc4c4c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_vectorizer = HashingVectorizer()\n",
    "X_hash = hash_vectorizer.fit_transform(fqdn_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "352363d6-4eb2-498d-9119-6864281bb008",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "X_count = count_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f94355d5-3884-42ed-85ce-8f2c07dda49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=0.6, min_samples=2, metric=\"cosine\").fit(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4b1e67b-7e1a-4b13-96f3-3ee71c74dd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 30\n",
      "Estimated number of noise points: 23\n"
     ]
    }
   ],
   "source": [
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9361be08-5904-48ca-b479-c3f75b88fa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated = defaultdict(list)\n",
    "for label, entry in zip(labels, cleaned_fqdn_docs):\n",
    "    translated[int(label)].append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12f1aab1-a84f-4ce5-b3b0-87d319400cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple-unlocked_com cpcalendars cpanel mail cpcontacts webdisk webmail ns2 www ns1',\n",
       " 'cpcalendars cpanel webdisk cpcontacts cashapp-mvf_com webmail www',\n",
       " 'cpcalendars webdisk cpcontacts webmail www cashapp-mvn_com',\n",
       " 'mx cpcalendars cpanel git web webdisk cpcontacts secure crm webdav cashapp-uid_com webmail www portal',\n",
       " 'cpcalendars cpanel cashappmvb-sec_com webdisk cpcontacts webmail www',\n",
       " 'cpcalendars cpanel cashapps-cid_com qa cpcontacts webdisk webmail www exchange',\n",
       " 'cpcalendars cpanel webdisk cpcontacts webmail cashapps-pid_com www poczta',\n",
       " 'cpcalendars cpanel webdisk cpcontacts webmail www cashappsalert-sec_com',\n",
       " 'cpcalendars cpanel mail webdisk cpcontacts dnb-no_com webmail www',\n",
       " 'cpcalendars cpanel webdisk cpcontacts mail webmail www eeukbtgroupjshzuayd7syz_com',\n",
       " 'wap cpcalendars notify-sms_com cpanel webdisk mail demo cpcontacts webmail www',\n",
       " 'psql02 mysql04 cpcalendars cpanel remote webdisk cpcontacts mail api recoverycash-cid_com share webmail www',\n",
       " 'cpcalendars cpanel webdisk cpcontacts webmail www recoverycash-pid_com',\n",
       " 'cpcalendars recoverycash-tid_com cpanel webdisk cpcontacts webmail www',\n",
       " 'cpcalendars cpanel mail webdisk cpcontacts img recoverycash-vid_com webmail www',\n",
       " 'cpcalendars talabat-help_com cpanel mail webdisk cpcontacts webmail ns2 www ns1',\n",
       " 'cpcalendars cpanel ulster-onlinenotice_com webdisk mail cpcontacts webmail www',\n",
       " 'cpanel git mail cpcontacts vpn bank webdisk webmail www westpaac_com',\n",
       " 'www mail zeero0ze_com']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = 3\n",
    "translated[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6029f454-5f7c-481d-9f5c-3657b7a6544f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['secure eqbanb_com',\n",
       " 'secure eqhanh_com',\n",
       " 'www secure eqhank_com webmail',\n",
       " 'secure roaylbamh_com',\n",
       " 'secure roaylbamk_com',\n",
       " 'royalhamh_com secure',\n",
       " 'secure royalhanh_com',\n",
       " 'secure royalkamb_com',\n",
       " 'royalkamh_com secure',\n",
       " 'royalkamk_com secure',\n",
       " 'www secure royalkanb_com',\n",
       " 'secure royalkanh_com',\n",
       " 'secure royalkank_com www1',\n",
       " 'seblv_com webmail ibanka',\n",
       " 'șẹb-lv_com ibanka',\n",
       " 'șẹb_com ibanka',\n",
       " 'șẹblv_com ibanka',\n",
       " 'șeb_com ibanka',\n",
       " 'șeb-lv_com ibanka',\n",
       " 'șeblv_com ibanka',\n",
       " 'eq-bạņks_com secure',\n",
       " 'www wp secure gȩtinhank_com',\n",
       " 'www secure gẹtinhạnk_com',\n",
       " 'www gȩtinhạnk_com secure',\n",
       " 'www wp kẹynaviqator-kẹy_com',\n",
       " 'sẹb-lv_com ibanka']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = 12\n",
    "translated[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac169f13-1e0b-4f74-b668-3bb511b58f58",
   "metadata": {},
   "source": [
    "Not bad. Not perfect...there's some weirdness about similar things ending up in different clusters. But I like where it's going with this. Going to pass this over to Clay to see what he thinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc7d0a7e-0a25-45d7-9904-65837bea5522",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"subdomain_clusters.json\", \"w\") as outfile:\n",
    "    json.dump(translated, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e989074e-e310-4ce7-9f82-9bc3b44461f8",
   "metadata": {},
   "source": [
    "Now, having these clusters, need to unwind the word-collection documents back to the fqdns that they came from. The way to do that, I *think*, is to walk through every subdomain, make a set of words from that subdomain (including changing the apex to the underscore version above), and check to see if that word is a subset of any of the entries in a cluster. If it's not, skip it. \n",
    "\n",
    "Things that will speed this up: make the \"translated\" defaultdict a defaultdict of sets, rather than list, and make the document a set also, rather than a list. This may just be a slog, though. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d032418-7828-4d7d-b570-3c64951a7caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_as_set = defaultdict(set)\n",
    "for key in translated:\n",
    "    translated_as_set[key] = [set(entry.split(\" \")) for entry in translated[key]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a836cc-cb35-4b4c-85a5-d5820df4ce95",
   "metadata": {},
   "source": [
    "now we re-do the walk through the dnsdb_resolutions, but don't add hte results to a document, instead write them to a new file with their correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89a9b999-1c20-4ab1-bd00-6de0aa23d097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cluster_id(words, translated_as_set):\n",
    "    for key in translated_as_set:\n",
    "        for entry in translated_as_set[key]:\n",
    "            if words.issubset(entry):\n",
    "                return key\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4fc30ef7-6bca-4f25-a064-090aab6d8b05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = {\"pnchạnbs_com\", \"www\", \"treasury\"}\n",
    "find_cluster_id(test, translated_as_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c320195-5b56-4acf-a797-3aa2f2b30ae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"subdomain_clusters.csv\", \"w\") as outfile:\n",
    "    outfile.write(f\"cluster_id,subdomain,apex_domain\\n\")\n",
    "    with open(\"dnsdb_resolutions.json\") as infile:\n",
    "        data = json.load(infile)\n",
    "        for base_domain in data:\n",
    "            if base_domain.endswith(\".\"):\n",
    "                substitute_domain = base_domain.rstrip(\".\")\n",
    "            else:\n",
    "                substitute_domain = base_domain\n",
    "            subdomains = set()\n",
    "            for entry in data[base_domain]['subdomains']:\n",
    "                subdomains.add(entry['domain'])\n",
    "            if len(subdomains) > 20:\n",
    "                continue\n",
    "            decoded_subdomains = set()\n",
    "            for entry in subdomains:\n",
    "                decoded_domain = decode_domain(entry)\n",
    "                if decoded_domain is None:\n",
    "                    continue\n",
    "                decoded_subdomains.add(decoded_domain)\n",
    "            underscore = substitute_domain.replace(\".\", \"_\")\n",
    "            for entry in decoded_subdomains:\n",
    "                replaced = entry.replace(substitute_domain, underscore)\n",
    "                parts = replaced.split(\".\")\n",
    "                parts_set = set(parts)\n",
    "                cluster_id = find_cluster_id(parts_set, translated_as_set)\n",
    "                if cluster_id is not None:\n",
    "                    outfile.write(f\"{cluster_id},{entry},{substitute_domain}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65424b02-2c40-40b9-b3a7-a00fd036817f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
