{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0be735e7-3e82-481c-a21e-6ac4d9100950",
   "metadata": {},
   "source": [
    "# Domain Name clustering\n",
    "\n",
    "Let's start from the top: we have a bunch of registrable domains on the disneyland team from a peer, Clay Blankenship. The question: can we cluster them together to find common targets for impersonation?\n",
    "\n",
    "First try: let's use the same technique used in the Sean/Kai webinar - use zipf's law splitting of the domains into words, and then cluster the domains by the words in the domain.\n",
    "\n",
    "If you haven't seen zipf's law before, it's an observational law that a number of distributions in nature are power laws - population of cities in the US, for example (8M, 4M, 2M for the top 3). You can use that with language (which also follows that power law), to predict which word combination is the most likely split of a word by finding the split that uses the most popular words first. We have a dictionary from wikipedia, and a splitter from StackOverflow, so here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cad661b-8613-47e7-b70f-603e75f7096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from zipf_split import ZipfSplitter\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66ba21f0-e920-4d4a-a03c-3a93fe0285cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFILE = \"disneyland_domains.json\"\n",
    "SCORE_FILE = \"zipf_dictionary.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "155f7b57-e552-4010-a02f-f8beaff07976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zipf_dict():\n",
    "    splitter = ZipfSplitter(SCORE_FILE)\n",
    "    return splitter\n",
    "\n",
    "def get_domains():\n",
    "    with open(INFILE) as infile:\n",
    "        data = json.load(infile)\n",
    "        domains = [entry[\"domain\"] for entry in data[\"response\"][\"results\"]]\n",
    "    return domains\n",
    "\n",
    "\n",
    "def split_domains(domains):\n",
    "    splitter = load_zipf_dict()\n",
    "    response = list()\n",
    "    for name in domains:\n",
    "        registrable_name = name.split(\".\")[0]\n",
    "        components = splitter.split_dns_name(registrable_name)\n",
    "        response.append(components)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0364e186-6587-4229-a37f-080195881c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = get_domains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc930d71-71cb-482d-b0eb-982eeb4fe8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_as_words = split_domains(domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b38a1f0-8322-4b9e-81cc-997846f28252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['26', 'uy', '6'], ['53', 'vb'], ['53', 'vl'], ['53', 'vz'], ['53', 'x', 'a']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domains_as_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef1c7793-b596-4667-8403-e37317e06376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['26uy6.top', '53vb.com', '53vl.com', '53vz.com', '53xa.com']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domains[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6b7aa72-eab6-44af-9a90-c402dba9f861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ẹxperss53.com',\n",
       " 'ẹxpness53.com',\n",
       " 'ẹxprass53.com',\n",
       " 'ẹxpres53.com',\n",
       " 'ẹxprses53.com',\n",
       " 'ẹxqress53.com',\n",
       " 'ẹxrpess53.com',\n",
       " 'zeero0ze.com',\n",
       " 'zionshamk.com',\n",
       " 'zlonshank.com']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domains[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0090b171-b998-459f-9c5b-1d26aa65635b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ẹ', 'x', 'p', 'e', 'r', 's', 's', '53'],\n",
       " ['ẹ', 'x', 'p', 'n', 'e', 's', 's', '53'],\n",
       " ['ẹ', 'x', 'p', 'r', 'a', 's', 's', '53'],\n",
       " ['ẹ', 'x', 'p', 'r', 'e', 's', '53'],\n",
       " ['ẹ', 'x', 'p', 'r', 's', 'e', 's', '53'],\n",
       " ['ẹ', 'x', 'q', 'r', 'e', 's', 's', '53'],\n",
       " ['ẹ', 'x', 'r', 'p', 'e', 's', 's', '53'],\n",
       " ['zeero', '0', 'ze'],\n",
       " ['zions', 'hamk'],\n",
       " ['zl', 'on', 'shank']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domains_as_words[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dee515-8645-4f52-8eb5-df987c8bfb40",
   "metadata": {},
   "source": [
    "Well, that didn't work. \n",
    "\n",
    "The problem: these aren't simple word splits. The domains here are homoglyphs and typos of words. The zipf splitting algorithm is having trouble because none of the strings in these domain names are dictionary words, so splitting on dictionary words isn't working. We'll have to clean up the homoglyphs before we can do any clustering. \n",
    "\n",
    "Next try:\n",
    " 1. build map of character substituions (this isn't a general answer, but probably better than nothing), build all possible new words substituting characters in, modify the zipf split to look for those. \n",
    " 2. Do clustering on the names, with a custom distance metric of the edit distance between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1070f195-6feb-4a82-8ecf-6ef86c680b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_map = {\n",
    "    'ạ': \"a\",\n",
    "    'ė': \"e\",\n",
    "    'ẹ': \"e\",\n",
    "    'ȩ': \"e\",\n",
    "    'ọ': \"o\",\n",
    "    'ņ': \"n\",\n",
    "    'ŗ': \"r\",\n",
    "    'ș': \"s\",\n",
    "    'ț': \"t\",\n",
    "    'ụ': \"u\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e91e0a1b-ecc8-4a86-9019-8854200cd25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_domains(domains):\n",
    "    response = list()\n",
    "    for domain in domains:\n",
    "        text = \"\".join([word_map.get(c, c) for c in domain])\n",
    "        response.append(text)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69a9e8f9-6d5d-4574-98e7-35e7f76c65f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_domains = clean_domains(domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc79287a-5b22-4136-a886-ab4ce411415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_domains_as_words = split_domains(cleaned_domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d312d729-dd4c-47d7-bf56-44d3648e6ab9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['experss', '53'],\n",
       " ['exp', 'ness', '53'],\n",
       " ['ex', 'prass', '53'],\n",
       " ['expres', '53'],\n",
       " ['exp', 'rses', '53'],\n",
       " ['exq', 'ress', '53'],\n",
       " ['exr', 'pess', '53'],\n",
       " ['zeero', '0', 'ze'],\n",
       " ['zions', 'hamk'],\n",
       " ['zl', 'on', 'shank']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_domains_as_words[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1892b38e-8d34-49bf-a766-bce864ff8ff7",
   "metadata": {},
   "source": [
    "This looks much better. Not perfect by any stretch, but better.\n",
    "\n",
    "So, now let's try clustering on that. The first try will be very simple: treat each domain name as a small \"document\", count the \"words\" in the document, and cluster the domains together by how many words they share. We'll do that with DBSCAN rather than K-Nearest Neighbors as DBSCAN is better suited to \"categorical\" data like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6862e3c-db81-4111-95e2-0bd4d39b5136",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "words_as_docs = [\" \".join(words) for words in cleaned_domains_as_words]\n",
    "X = vectorizer.fit_transform(words_as_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a0ef6c2-f644-4d3a-bd54-e40acb2cc637",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=0.3, min_samples=3).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73c6f82a-8ea1-4f68-b4c3-88e7cf0ce8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 17\n",
      "Estimated number of noise points: 405\n"
     ]
    }
   ],
   "source": [
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704c403f-24b9-42ca-a805-c4b0a1d50713",
   "metadata": {},
   "source": [
    "One thing about DBSCAN: it has a \"none of the above\" cluster, where the entries that didn't fit into any other cluster go. That \"noise\" cluster is huge in this case: 405 of the 494 domains didn't cluster. That's pretty poor. Just for argument's sake, let's look at what it did find, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cd8fbf4-5e1a-475a-ae93-6e8cc34e8f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1cb88a5-c7c9-4e15-b0c2-56d24cd5aeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, entry in zip(labels, words_as_docs):\n",
    "    translated[label].append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb95227c-3216-41ba-a622-2f0ea3cde4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0ef8bb3-d62d-48bc-af0b-a88b6a1e30b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['key navl gators key',\n",
       " 'key navl gators key',\n",
       " 'key navl gators key',\n",
       " 'key navl gators key']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = 15\n",
    "translated[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf15efaf-72f8-46a4-974c-c4bfe370abe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sebl v', 'sebl v', 'sebl v']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key=4\n",
    "translated[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e761b5-088f-48a6-a54e-6a099572cf8a",
   "metadata": {},
   "source": [
    "It sort of worked, but not well enough to be useful. We expected this to be naive, and it is. It's not working super well with typos, and the like. The thing is, even after cleaning up the homoglyphs, few of the words in the clusters are real english words, so clustering on them isn't working. Also, since they're not real english words, trying any other fancy language processing like Lemmatization won't work since those need dictionary words.\n",
    "\n",
    "This approach isn't working. More likely we need to look into a way to cluster the homoglyph'd & typo'd domains together somehow. On to the next experiment (see `Homoglyph identification.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffe0b3d-06a2-4980-9424-a28b9e873780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general-jupyter",
   "language": "python",
   "name": "general-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
