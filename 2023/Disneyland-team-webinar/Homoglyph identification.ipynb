{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6edc9d9-b944-4750-9dfc-0336a93ba34d",
   "metadata": {},
   "source": [
    "# Homoglyph identification\n",
    "\n",
    "Round 2: Since so many of the domains in the Disneyland dataset appear to be homoglyph attacks, I think this analysis needs to start with decoding the homoglyphs.\n",
    "\n",
    "In the earlier notebook we tried simple substitutions for the letters, I'd like to try something more general (though much more computationally intensive): one repeating idea I've seen is to render the entry to an image, then do something like OCR with only ascii letters to see what it finds. let's see if we can get that working here.\n",
    "\n",
    "The idea here is to make a picture of the word, blur the word slightly, and then OCR it back to ASCII text. The hope is that bluring the dots over/under the letters will make them appear to be noise on the letter, and discarded by the OCR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e13be7b-6837-4f98-8f84-40c8e4ec7b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageFilter\n",
    "import pytesseract\n",
    "from leven import levenshtein\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdcc5665-a26b-45f0-878f-3c9cf43b21e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFILE = \"disneyland_domains.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddb208b8-13f2-44b7-96c2-92d9730f5d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domains():\n",
    "    with open(INFILE) as infile:\n",
    "        data = json.load(infile)\n",
    "        domains = [entry[\"domain\"] for entry in data[\"response\"][\"results\"]]\n",
    "    return domains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9bf4da3-3da1-440a-a0a6-b655e97e76d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.new(\"RGB\", (1000,50), (255,255,255))\n",
    "font = ImageFont.truetype(\"Arial.ttf\", 24)\n",
    "drawer = ImageDraw.Draw(img)\n",
    "drawer.text((5,5), \"suncoastcrẹditunlọn.com\", fill=(0,0,0), font=font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ab8793b-e4e2-428d-bff5-45b51dede815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'suncoastcreditunlon.com\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytesseract.image_to_string(img, lang=\"eng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17c24d67-db1a-4d7f-a232-8493d36c8010",
   "metadata": {},
   "outputs": [],
   "source": [
    "im2 = img.filter(ImageFilter.GaussianBlur(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03602f9a-cd40-47bc-a782-1f5b4969adf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suncoastcreditunion.com\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pytesseract.image_to_string(im2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab7c4bb-702d-4666-bec2-f2c38dbe8f07",
   "metadata": {},
   "source": [
    "That's perfect. That is doing exactly what I want it to: translating the \"l\" to an i, dropping the dots over and under letters. Let's see about applying that to the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dbd6ee2-ac79-40db-ad5d-b766bf55b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_domain(domain):\n",
    "    img = Image.new(\"RGB\", (1000,50), (255,255,255))\n",
    "    font = ImageFont.truetype(\"Arial.ttf\", 24)\n",
    "    drawer = ImageDraw.Draw(img)\n",
    "    drawer.text((5,5), domain, fill=(0,0,0), font=font)\n",
    "    im2 = img.filter(ImageFilter.GaussianBlur(1.0))\n",
    "    return pytesseract.image_to_string(im2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deb4893c-93e6-431c-84f9-66d1aaec8edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_domains = get_domains()\n",
    "fixed_domains = list()\n",
    "for entry in raw_domains:\n",
    "    translated = translate_domain(entry)\n",
    "    translated = translated.strip()\n",
    "    if not translated:\n",
    "        fixed_domains.append(entry)\n",
    "    else:\n",
    "        fixed_domains.append(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4410262-437e-4dc6-af85-730bc7a58c12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['experss53.com',\n",
       " 'expness53.com',\n",
       " 'exprass53.com',\n",
       " 'expres53.com',\n",
       " 'exprses53.com',\n",
       " 'exqress53.com',\n",
       " 'exrpess53.com',\n",
       " 'zeero0ze.com',\n",
       " 'zionshamk.com',\n",
       " 'zlonshank.com']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_domains[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d91cb8f-f544-41bc-b9ab-7b1d24b4ff9d",
   "metadata": {},
   "source": [
    "Much better. We've at least got these down to typos now, rather than unicode-homoglyphs, which is a great start. Now, let's try clustering the typo domains. \n",
    "\n",
    "It's worth noting that this takes a few minutes to go through all the domains, so this isn't a great solution for the case where you have many thousands of these, or where you need to do them in near real time. \n",
    "\n",
    "Anyway, with that fixed, on to clustering. Last time, we tried this with just word counts, and that didn't work. This time, we'll use the levenshtein distance (edit distance) as the metric for clustering, so domains that require fewer changes to get from one string to another will be considered \"close\", while ones that require more letter changes will be considered \"far\" apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1b5326e-e905-4993-b79e-91b878f9f793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lev_metric(x, y):\n",
    "    i, j = int(x[0]), int(y[0])     # extract indices, see comment about X below\n",
    "    # pull just the registrable domain name, not the eTLD\n",
    "    first = fixed_domains[i].split(\".\", 1)[0]\n",
    "    second = fixed_domains[j].split(\".\", 1)[0]\n",
    "    return levenshtein(first, second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbaa6ed1-b0c4-4af6-9b2b-82fb166a019d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levenshtein(fixed_domains[-1].split(\".\", 1)[0], fixed_domains[-2].split(\".\", 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b27acca7-9524-44e1-84ad-83212c220d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this case, we just make the input to the clustering alg a set of array indexes, and let the lev_metric method grab them to \n",
    "# do the comparisons\n",
    "X = np.arange(len(fixed_domains)).reshape(-1, 1)\n",
    "db = DBSCAN(eps=2, metric=lev_metric, min_samples=2).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d7a5a6-64d4-4e09-be3e-5083cc33a7a4",
   "metadata": {},
   "source": [
    "It's worth mentioning here that we chose `eps` here intentionally to be the edit distance of 2. So, we're allowing words with up to 2 letter differences between them to be in the same cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76559a64-2e01-4df0-b457-b21d124eefaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 57\n",
      "Estimated number of noise points: 37\n"
     ]
    }
   ],
   "source": [
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715e66b7-eb29-4b38-a4bf-217af701fdc6",
   "metadata": {},
   "source": [
    "This is more like it. only 37 things in the noise category, and 57 other clusters. Let's look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08380d09-a0b6-4755-8a1b-3fc09cde4c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated = defaultdict(list)\n",
    "for label, entry in zip(labels, raw_domains):\n",
    "    translated[int(label)].append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd65bc5c-33e3-4199-9b19-49ff67666d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cwbanb.com',\n",
       " 'cwhanb.com',\n",
       " 'cwhanh.com',\n",
       " 'cwhank.com',\n",
       " 'cwkanh.com',\n",
       " 'cwkank.com',\n",
       " 'eqbanb.com',\n",
       " 'eqhanh.com',\n",
       " 'eqhank.com',\n",
       " 'tdhank.com',\n",
       " 'ụșbamh.com',\n",
       " 'ụșbamk.com',\n",
       " 'ụșbanh.com',\n",
       " 'ụșbbanh.com',\n",
       " 'ụșbhanh.com',\n",
       " 'ụșhaank.com',\n",
       " 'ụșhamk.com',\n",
       " 'ușbhạnk.com',\n",
       " 'ușbạạnk.com',\n",
       " 'ușhạmk.com',\n",
       " 'ușhạnk.com',\n",
       " 'usbhạnk.com',\n",
       " 'ushaạnk.com']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = 12\n",
    "translated[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77e837ad-02ee-49d0-aa61-048c889a4932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sẹotiaomline-sẹotiahank.com',\n",
       " 'sẹotiaonline-sẹotiahank.com',\n",
       " 'sẹotiaonline-sẹotlahank.com']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key=55\n",
    "translated[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7440451f-f73b-4373-921c-2c93cdc97e29",
   "metadata": {},
   "source": [
    "Beautiful. That works really well. The scotiaonline-scotiabank ones clustered seperately to the scotiabank only ones, but for this exercise, that's going to be really hard to fix. (Doing both typos and internal word similarity will be hard.) I'm going to declare victory on this exercise and move on.\n",
    "\n",
    "Let's export this back out so that we can hand it back to Clay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aba62e10-535f-4967-b0b6-34834e43a95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(translated, open(\"disney-clusters.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6aea3b-4b03-4173-99d0-465c0beef84b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general-jupyter",
   "language": "python",
   "name": "general-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
